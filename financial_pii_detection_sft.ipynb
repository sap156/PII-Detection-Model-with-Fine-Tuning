{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf_khmJeX5qT"
      },
      "source": [
        "# Financial PII Detection Model - Supervised Fine-Tuning\n",
        "\n",
        "This notebook demonstrates how to fine-tune a language model to detect and protect personally identifiable information (PII) in financial documents.\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, make sure you're using a GPU runtime in Colab:\n",
        "- Go to Runtime > Change runtime type\n",
        "- Select GPU from the Hardware accelerator dropdown\n",
        "- Click Save"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpWQdNpCYIwV"
      },
      "source": [
        "## Step 1: Install Unsloth\n",
        "\n",
        "Unsloth is a library that makes fine-tuning faster and more memory-efficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0C7hO3oX3w8"
      },
      "outputs": [],
      "source": [
        "!pip install unsloth -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1X1nKMWXYOfC"
      },
      "source": [
        "## Step 2: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-Xst-yrYPbR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sctLWNbGYTuB"
      },
      "source": [
        "## Step 3: Set Configuration Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8hGPGhVYYPB"
      },
      "outputs": [],
      "source": [
        "max_seq_length = 2048  # Maximum number of tokens in a sequence\n",
        "dtype = None           # Data type for model weights (None uses default)\n",
        "load_in_4bit = True    # Enable 4-bit quantization for memory efficiency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xtx6oKTbYaYy"
      },
      "source": [
        "## Step 4: Log in to Hugging Face\n",
        "\n",
        "You'll need to add your Hugging Face token to your Google Colab secrets:\n",
        "1. Go to https://huggingface.co/settings/tokens to create a token if you don't have one\n",
        "2. In Colab, click on the key icon in the left sidebar\n",
        "3. Add a new secret with name \"HuggingFace\" and your token as the value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YcO4bwbYklB"
      },
      "outputs": [],
      "source": [
        "hf_token = userdata.get('HuggingFace')\n",
        "login(hf_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjxGKvGLYmp5"
      },
      "source": [
        "## Step 5: Load the Model and Tokenizer\n",
        "\n",
        "We're using the DeepSeek-R1-Distill-Llama-8B model, which is a good balance of power and efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_9p9zGFYqTB"
      },
      "outputs": [],
      "source": [
        "print(\"Loading base model...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    token = hf_token,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YjWcppQYrzi"
      },
      "source": [
        "## Step 6: Define the Prompt Formats\n",
        "\n",
        "We'll create two prompt formats:\n",
        "- One for inference (testing)\n",
        "- One for training that includes the \"think\" tags for chain-of-thought reasoning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXfvDh-gYu4J"
      },
      "outputs": [],
      "source": [
        "# Format for inference\n",
        "prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "\n",
        "Write a response that appropriately completes the request.\n",
        "\n",
        "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
        "\n",
        "### Instruction:\n",
        "\n",
        "You are a financial expert specializing in identifying and protecting personally identifiable information (PII) in financial documents.\n",
        "\n",
        "Please analyze the following document for any PII and explain which elements need protection.\n",
        "\n",
        "### Document:\n",
        "\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "\n",
        "{}\"\"\"\n",
        "\n",
        "# Format for training (with think tags)\n",
        "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "\n",
        "Write a response that appropriately completes the request.\n",
        "\n",
        "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
        "\n",
        "### Instruction:\n",
        "\n",
        "You are a financial expert specializing in identifying and protecting personally identifiable information (PII) in financial documents.\n",
        "\n",
        "Please analyze the following document for any PII and explain which elements need protection.\n",
        "\n",
        "### Document:\n",
        "\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "\n",
        "<think>\n",
        "\n",
        "{}\n",
        "\n",
        "</think>\n",
        "\n",
        "{}\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v8wr7_sY8PZ"
      },
      "source": [
        "## Step 7: Test the Base Model Before Fine-Tuning\n",
        "\n",
        "Let's see how the model performs on a sample financial document before training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRZiVGUHZBfp"
      },
      "outputs": [],
      "source": [
        "print(\"Testing base model...\")\n",
        "test_document = \"\"\"\n",
        "Loan Application\n",
        "\n",
        "Full Legal Name: Jane A. Smith\n",
        "Date of Birth: 04/12/1990\n",
        "\n",
        "Mailing Address:\n",
        "123 Main Street\n",
        "Boston, MA 02108\n",
        "\n",
        "Phone Number: (617) 555-1234\n",
        "Email Address: jane.smith@email.com\n",
        "\n",
        "Bank Account: 9876543210\n",
        "Social Security Number: 123-45-6789\n",
        "\"\"\"\n",
        "\n",
        "FastLanguageModel.for_inference(model) \n",
        "inputs = tokenizer([prompt_style.format(test_document, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs.input_ids,\n",
        "    attention_mask=inputs.attention_mask,\n",
        "    max_new_tokens=1200,\n",
        "    use_cache=True,\n",
        ")\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "print(\"Base model response:\")\n",
        "print(response[0].split(\"### Response:\")[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOa-fmTXZJH5"
      },
      "source": [
        "## Step 8: Initialize the Model for Fine-Tuning\n",
        "\n",
        "We'll use LoRA (Low-Rank Adaptation) to efficiently fine-tune the model while keeping memory usage low."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtHE1qIVZNh5"
      },
      "outputs": [],
      "source": [
        "print(\"Initializing for fine-tuning...\")\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,                   # Rank of the adaptation matrices\n",
        "    target_modules=[        # Which modules to fine-tune\n",
        "        \"q_proj\",           # Query projection\n",
        "        \"k_proj\",           # Key projection\n",
        "        \"v_proj\",           # Value projection\n",
        "        \"o_proj\",           # Output projection\n",
        "        \"gate_proj\",        # Gate projection for MLP\n",
        "        \"up_proj\",          # Upward projection for MLP\n",
        "        \"down_proj\",        # Downward projection for MLP\n",
        "    ],\n",
        "    lora_alpha=16,          # Alpha parameter for LoRA\n",
        "    lora_dropout=0,         # Dropout probability for LoRA\n",
        "    bias=\"none\",            # Whether to train bias parameters\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Memory optimization\n",
        "    random_state=9001,      # Random seed for reproducibility\n",
        "    use_rslora=False,       # Whether to use rank-stabilized LoRA\n",
        "    loftq_config=None,      # LoftQ quantization config\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVKX-_qpZSQZ"
      },
      "source": [
        "## Step 9: Create the Formatting Function for Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pP4TmfHRZUXh"
      },
      "outputs": [],
      "source": [
        "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    inputs = examples[\"text\"]\n",
        "    texts = []\n",
        "    \n",
        "    for input_text in inputs:\n",
        "        # For the reasoning part, we'll use empty strings during training\n",
        "        # The model will learn to generate this during fine-tuning\n",
        "        cot = \"\"\n",
        "        \n",
        "        # Create simple output that identifies PII\n",
        "        output = \"This document contains personally identifiable information (PII) that should be protected according to financial regulations.\"\n",
        "        \n",
        "        # Format the text according to our training prompt style\n",
        "        text = train_prompt_style.format(input_text, cot, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "        \n",
        "    return {\"text\": texts}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp1A-iuLZYah"
      },
      "source": [
        "## Step 10: Load and Prepare the Dataset\n",
        "\n",
        "We'll use the Gretel AI synthetic financial PII dataset from Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooFXhkxKZbqh"
      },
      "outputs": [],
      "source": [
        "print(\"Loading dataset...\")\n",
        "dataset = load_dataset(\"gretelai/synthetic_pii_finance_multilingual\", split=\"train[0:500]\")\n",
        "dataset = dataset.filter(lambda example: example[\"language\"] == \"English\")  # Optional: filter to just English documents\n",
        "\n",
        "print(\"Preparing dataset...\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# Print a sample to verify\n",
        "print(\"Sample formatted training data:\")\n",
        "print(dataset[\"text\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WBXVMucZfhh"
      },
      "source": [
        "## Step 11: Set Up the Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5b_Wm2rZg_Z"
      },
      "outputs": [],
      "source": [
        "print(\"Setting up trainer...\")\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,      # Number of examples per GPU\n",
        "        gradient_accumulation_steps=4,      # Number of updates to accumulate before updating weights\n",
        "        warmup_steps=5,                     # Steps of warmup for learning rate\n",
        "        max_steps=60,                       # Total number of training steps\n",
        "        learning_rate=2e-4,                 # Learning rate\n",
        "        fp16=not is_bfloat16_supported(),   # Whether to use 16-bit floating point precision\n",
        "        bf16=is_bfloat16_supported(),       # Whether to use bfloat16 precision (better on newer GPUs)\n",
        "        logging_steps=10,                   # How often to log stats\n",
        "        optim=\"adamw_8bit\",                 # Optimizer to use\n",
        "        weight_decay=0.01,                  # L2 regularization strength\n",
        "        lr_scheduler_type=\"linear\",         # Learning rate schedule\n",
        "        seed=3407,                          # Random seed\n",
        "        output_dir=\"outputs\",               # Directory to save outputs\n",
        "        report_to=\"none\"                    # Disable reporting to services like Wandb\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5X8L7XsZml5"
      },
      "source": [
        "## Step 12: Train the Model\n",
        "\n",
        "This is the main training step. For a quick demonstration, we're only using 60 steps, but for production use, you'd want to increase this to 500-2000 steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYH_8Cf6ZqVZ"
      },
      "outputs": [],
      "source": [
        "print(\"Starting fine-tuning...\")\n",
        "trainer_stats = trainer.train()\n",
        "print(\"Fine-tuning complete!\")\n",
        "print(f\"Training stats: {trainer_stats}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pSpdgJ0Zreh"
      },
      "source": [
        "## Step 13: Test the Fine-Tuned Model\n",
        "\n",
        "Let's see how the model performs on the same document after training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MptgIrdeZuyB"
      },
      "outputs": [],
      "source": [
        "print(\"Testing fine-tuned model...\")\n",
        "FastLanguageModel.for_inference(model)\n",
        "inputs = tokenizer([prompt_style.format(test_document, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs.input_ids,\n",
        "    attention_mask=inputs.attention_mask,\n",
        "    max_new_tokens=1200,\n",
        "    use_cache=True,\n",
        ")\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "print(\"Fine-tuned model response:\")\n",
        "print(response[0].split(\"### Response:\")[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJIJTpq2ZwPh"
      },
      "source": [
        "## Step 14: Save the Model\n",
        "\n",
        "Now that our model is trained, let's save it locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PH3p1N6iZzAJ"
      },
      "outputs": [],
      "source": [
        "print(\"Saving model...\")\n",
        "new_model_local = \"Financial-PII-Detection-Expert\"\n",
        "\n",
        "model.save_pretrained(new_model_local)\n",
        "tokenizer.save_pretrained(new_model_local)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDjpf83lZ1eR"
      },
      "source": [
        "## Step 15: Push to Hugging Face (Optional)\n",
        "\n",
        "If you want to share your model with others, you can upload it to Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Dq4iPm9Z4rx"
      },
      "outputs": [],
      "source": [
        "# Set to True if you want to push to Hugging Face\n",
        "push_to_hub = False\n",
        "\n",
        "if push_to_hub:\n",
        "    print(\"Pushing to Hugging Face...\")\n",
        "    # Change to your username\n",
        "    new_model_online = \"your-username/financial-pii-detection-expert\"  \n",
        "    \n",
        "    model.push_to_hub(new_model_online)\n",
        "    tokenizer.push_to_hub(new_model_online)\n",
        "    print(f\"Model pushed to: {new_model_online}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNyHlxEZZ8eh"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Congratulations! You've successfully fine-tuned a language model to identify and protect PII in financial documents. This model can now be used as part of your data privacy workflow.\n",
        "\n",
        "Some ideas for how to use this model:\n",
        "- Create an automated PII detection system for document processing\n",
        "- Build a compliance checking tool for financial documents\n",
        "- Use it as a training tool for staff handling sensitive information\n",
        "- Integrate it into a document redaction pipeline"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPRmfbLMEWkxFjJjf+Xb9Rj",
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
